{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
    "\n",
    "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
    "客製化資料讀取。\n",
    "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
    "(請同學先行至IMDB下載資料)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import torch and other required modules\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #下載stopwords\n",
    "nltk.download('punkt') #下載word_tokenize需要的corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索資料與資料前處理\n",
    "這份作業我們使用test資料中的pos與neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length before removing stopwords: 89527\n",
      "vocab length after removing stopwords: 89356\n"
     ]
    }
   ],
   "source": [
    "# 讀取字典，這份字典為review內所有出現的字詞\n",
    "###<your code>###\n",
    "with open('.\\\\aclImdb\\\\imdb.vocab','r',encoding = 'utf-8') as f:\n",
    "    vocab = f.read()\n",
    "\n",
    "vocab = vocab.split('\\n')\n",
    "\n",
    "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
    "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
    "vocab = list(set(vocab).difference(stopwords.words('english')))\n",
    "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
    "\n",
    "# 將字典轉換成dictionary\n",
    "### <your code> ###\n",
    "vocab_dict = dict(zip(vocab,range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.\\\\aclImdb\\\\train\\\\pos\\\\0_9.txt', 1), ('.\\\\aclImdb\\\\train\\\\pos\\\\10000_8.txt', 1)]\n",
      "Total reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
    "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
    "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
    "\n",
    "### <your code> ###\n",
    "pos = glob.glob(\".\\\\aclImdb\\\\train\\\\pos\\\\*.txt\")\n",
    "neg = glob.glob(\"./aclImdb/test/neg/*.txt\")\n",
    "review = pos + neg\n",
    "y =[1]*len(pos)+[0]*len(neg)\n",
    "\n",
    "review_pairs = list(zip(review,y))\n",
    "\n",
    "print(review_pairs[:2])\n",
    "print(f\"Total reviews: {len(review_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
    "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
    "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review(review_path):\n",
    "    \n",
    "    ###<your code>###\n",
    "    \n",
    "    with open(review_path,\"r\",encoding = \"utf-8\") as f:\n",
    "        review = f.read()\n",
    "    \n",
    "    #移除non-alphabet符號、贅字與tokenize\n",
    "    ###<your code>###\n",
    "    review = re.sub('[^A-Za-z]',' ',review)\n",
    "    review = nltk.word_tokenize(review)\n",
    "    review = set(review).difference(set(stopwords.words('english')))\n",
    "    \n",
    "    return review    \n",
    "    \n",
    "\n",
    "def generate_vec(review, vocab_dict):\n",
    "    ### <your code> ###\n",
    "    bag_vector = []\n",
    "    for word in review:\n",
    "        if vocab_dict.get(word):\n",
    "            bag_vector.append(vocab_dict.get(word))\n",
    "            \n",
    "    return torch.tensor(bag_vector)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立客製化dataset\n",
    "\n",
    "class dataset(Dataset):\n",
    "    '''custom dataset to load reviews and labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pairs: list\n",
    "        directory of all review-label pairs\n",
    "    vocab: list\n",
    "        list of vocabularies\n",
    "    '''\n",
    "    ### <your code> ###\n",
    "    def __init__(self, data_dirs, vocab):\n",
    "        ###<your code>###\n",
    "        self.data_dirs = data_dirs\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        ###<your code>###\n",
    "        return len(self.data_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ###<your code>###\n",
    "        pairs = self.data_dirs[idx]\n",
    "        review = pairs[0]\n",
    "        review = load_review(review)\n",
    "        review = generate_vec(review,self.vocab)\n",
    "        return review, pairs[1]    \n",
    "\n",
    "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
    "def collate_fn(batch):\n",
    "    ### <your code> ###\n",
    "    corpus, labels = zip(*batch)\n",
    "    lengths = [len(x) for x in corpus]\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    batch_corpus = []\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        tmp_pads = torch.zeros(max_len)\n",
    "        tmp_pads[:lengths[i]] = corpus[i]\n",
    "#         tmp_pads.view(-1,1)\n",
    "        batch_corpus.append(tmp_pads.view(1,-1))\n",
    "    \n",
    "    return torch.cat(batch_corpus,dim=0), torch.tensor(labels), torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7.6628e+04, 1.0326e+04, 8.3439e+04, 7.2104e+04, 2.4067e+04, 5.9449e+04,\n",
       "          8.0000e+00, 1.7700e+02, 2.9560e+03, 4.8474e+04, 1.8296e+04, 6.7040e+04,\n",
       "          1.5480e+04, 4.4871e+04, 8.5534e+04, 3.5854e+04, 7.2307e+04, 7.7381e+04,\n",
       "          1.3140e+04, 1.2077e+04, 3.6031e+04, 3.9798e+04, 8.6811e+04, 1.1585e+04,\n",
       "          6.2504e+04, 4.1248e+04, 4.8188e+04, 5.7546e+04, 1.3167e+04, 6.8749e+04,\n",
       "          6.4633e+04, 5.4835e+04, 7.9638e+04, 7.8491e+04, 5.6400e+02, 3.3133e+04,\n",
       "          5.5049e+04, 2.4451e+04, 1.1313e+04, 7.2182e+04, 6.0592e+04, 1.0099e+04,\n",
       "          5.1566e+04, 6.8276e+04, 5.6151e+04, 6.8113e+04, 1.3933e+04, 3.6468e+04,\n",
       "          6.5205e+04, 7.8519e+04, 6.7115e+04, 5.3448e+04, 5.2419e+04, 7.8979e+04,\n",
       "          3.3685e+04, 6.3281e+04, 6.9963e+04, 8.6260e+03, 2.4008e+04, 7.7483e+04,\n",
       "          9.4850e+03, 5.5811e+04, 7.2925e+04, 1.2180e+04, 4.0607e+04, 7.6430e+04,\n",
       "          6.2360e+03, 4.0976e+04, 7.4320e+03, 6.0979e+04, 2.3853e+04, 6.2415e+04,\n",
       "          8.4623e+04, 7.7328e+04, 6.1869e+04, 7.6603e+04, 5.0216e+04, 1.8776e+04,\n",
       "          8.2750e+04, 7.1887e+04, 2.4523e+04, 6.3686e+04, 4.3625e+04, 5.2997e+04,\n",
       "          5.1647e+04, 5.8028e+04, 6.5100e+02, 7.9820e+03, 2.5750e+03, 3.8008e+04,\n",
       "          5.5866e+04, 3.7110e+04],\n",
       "         [7.6628e+04, 3.7836e+04, 8.6096e+04, 4.8474e+04, 7.3991e+04, 5.8061e+04,\n",
       "          4.6078e+04, 8.2450e+04, 7.1190e+03, 6.9450e+03, 4.3492e+04, 8.3530e+03,\n",
       "          6.9600e+02, 6.4603e+04, 1.1585e+04, 6.2683e+04, 7.6015e+04, 2.5101e+04,\n",
       "          8.3690e+03, 7.1102e+04, 4.4348e+04, 4.9761e+04, 1.5332e+04, 4.7270e+04,\n",
       "          5.4654e+04, 1.0388e+04, 7.7593e+04, 1.6572e+04, 5.1718e+04, 8.2487e+04,\n",
       "          4.0360e+04, 7.8491e+04, 1.9418e+04, 2.5278e+04, 7.8307e+04, 1.1964e+04,\n",
       "          6.4130e+04, 5.5448e+04, 5.2064e+04, 4.1113e+04, 3.5407e+04, 4.1984e+04,\n",
       "          3.4891e+04, 8.5808e+04, 4.7810e+03, 3.0663e+04, 1.3942e+04, 5.3448e+04,\n",
       "          6.9963e+04, 4.4850e+03, 5.3320e+03, 6.0370e+03, 8.2190e+04, 4.0607e+04,\n",
       "          4.8412e+04, 1.3971e+04, 6.2200e+02, 1.8320e+03, 7.4686e+04, 7.7126e+04,\n",
       "          2.2605e+04, 1.8460e+03, 1.0645e+04, 8.6261e+04, 3.4443e+04, 2.4523e+04,\n",
       "          1.9845e+04, 8.8430e+03, 5.4033e+04, 2.4217e+04, 2.9348e+04, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]]),\n",
       " tensor([1, 0]),\n",
       " tensor([92, 71]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
    "### <your code> ###\n",
    "custom_dst = dataset(review_pairs,vocab_dict)\n",
    "custom_dataloader = DataLoader(custom_dst,collate_fn=collate_fn,batch_size=2,shuffle = True)\n",
    "next(iter(custom_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
