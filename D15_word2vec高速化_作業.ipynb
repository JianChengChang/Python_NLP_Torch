{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Pf_RxOIAYv"
   },
   "source": [
    "### 作業目的: 透過實作加速版word2vec Skip-gram模型來更加了解高速版的word2vec\n",
    "\n",
    "本次作業會採用Penn Tree Bank資料及，學員可以在ptb.train.txt中取得訓練文本資料。這次作業可以讓學員練習到以pytorch搭建模型與進行文本資料的前處理\n",
    "\n",
    "PS: 建議學員使用Colab (或可以使用GPU加速的機器)來進行作業，不然訓練會訓練到天荒地老....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO-a6e2OI5zg"
   },
   "source": [
    "### Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 14937,
     "status": "ok",
     "timestamp": 1606320756664,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "LXPU7BI3HNJ6"
   },
   "outputs": [],
   "source": [
    "# Import libraries for importing files from Google drive to Colab\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authorize Google SDK to access Google Drive from Colab\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 17072,
     "status": "ok",
     "timestamp": 1606320758810,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "D2E7yb-qI9Uv"
   },
   "outputs": [],
   "source": [
    "download = drive.CreateFile({'id': '請自行輸入自己上傳google drive檔案的連結id'})\n",
    "download.GetContentFile('ptb.train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKKpFV6GJwhs"
   },
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4149,
     "status": "ok",
     "timestamp": 1606320764926,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "Yjz-fWmbJRPB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import urllib.request\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3426,
     "status": "ok",
     "timestamp": 1606320764930,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "i9xrgPu3KBgJ",
    "outputId": "341dcbac-256c-45ee-ed8d-3ee0c1fb03b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 42068 lines\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料\n",
    "\n",
    "# Penn Tree Back dataset\n",
    "with open(\"./ptb.train.txt\", encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(f\"Total {len(lines)} lines\")\n",
    "raw_dataset = [line.split() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2988,
     "status": "ok",
     "timestamp": 1606320764931,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "oAcF_5CQKH_J",
    "outputId": "b42ef993-9894-4061-f8df-3f929fe17114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aer',\n",
       "  'banknote',\n",
       "  'berlitz',\n",
       "  'calloway',\n",
       "  'centrust',\n",
       "  'cluett',\n",
       "  'fromstein',\n",
       "  'gitano',\n",
       "  'guterman',\n",
       "  'hydro-quebec',\n",
       "  'ipo',\n",
       "  'kia',\n",
       "  'memotec',\n",
       "  'mlx',\n",
       "  'nahb',\n",
       "  'punts',\n",
       "  'rake',\n",
       "  'regatta',\n",
       "  'rubens',\n",
       "  'sim',\n",
       "  'snack-food',\n",
       "  'ssangyong',\n",
       "  'swapo',\n",
       "  'wachter'],\n",
       " ['pierre',\n",
       "  '<unk>',\n",
       "  'N',\n",
       "  'years',\n",
       "  'old',\n",
       "  'will',\n",
       "  'join',\n",
       "  'the',\n",
       "  'board',\n",
       "  'as',\n",
       "  'a',\n",
       "  'nonexecutive',\n",
       "  'director',\n",
       "  'nov.',\n",
       "  'N'],\n",
       " ['mr.',\n",
       "  '<unk>',\n",
       "  'is',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'n.v.',\n",
       "  'the',\n",
       "  'dutch',\n",
       "  'publishing',\n",
       "  'group'],\n",
       " ['rudolph',\n",
       "  '<unk>',\n",
       "  'N',\n",
       "  'years',\n",
       "  'old',\n",
       "  'and',\n",
       "  'former',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  'consolidated',\n",
       "  'gold',\n",
       "  'fields',\n",
       "  'plc',\n",
       "  'was',\n",
       "  'named',\n",
       "  'a',\n",
       "  'nonexecutive',\n",
       "  'director',\n",
       "  'of',\n",
       "  'this',\n",
       "  'british',\n",
       "  'industrial',\n",
       "  'conglomerate'],\n",
       " ['a',\n",
       "  'form',\n",
       "  'of',\n",
       "  'asbestos',\n",
       "  'once',\n",
       "  'used',\n",
       "  'to',\n",
       "  'make',\n",
       "  'kent',\n",
       "  'cigarette',\n",
       "  'filters',\n",
       "  'has',\n",
       "  'caused',\n",
       "  'a',\n",
       "  'high',\n",
       "  'percentage',\n",
       "  'of',\n",
       "  'cancer',\n",
       "  'deaths',\n",
       "  'among',\n",
       "  'a',\n",
       "  'group',\n",
       "  'of',\n",
       "  'workers',\n",
       "  'exposed',\n",
       "  'to',\n",
       "  'it',\n",
       "  'more',\n",
       "  'than',\n",
       "  'N',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'researchers',\n",
       "  'reported']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看前5筆\n",
    "raw_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3502,
     "status": "ok",
     "timestamp": 1606320765980,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "3oki6AxhJyj4",
    "outputId": "79a06d23-2176-4600-b55b-033dfb81ce49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before subsampling: 885720 words\n",
      "After subsampling: 448364 words\n"
     ]
    }
   ],
   "source": [
    "# 定義資料前處理函示\n",
    "class PreProcessor():\n",
    "    '''Function to do preprocess of input corpus\n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: str\n",
    "        input corpus to be processed\n",
    "    only_word: bool\n",
    "        whether to filter out non-word\n",
    "    min_freq: int\n",
    "        minimum frequency of a word to be kept\n",
    "    do_subsampling: bool\n",
    "        whether to do subsampling\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, only_word: bool=False, min_freq: int=5, do_subsampling: bool=True, t: float=1e-5):\n",
    "        self.only_word = only_word\n",
    "        self.min_freq = min_freq\n",
    "        self.do_subsampling = do_subsampling\n",
    "        self.t = t\n",
    "    \n",
    "    def process(self, corpus: List[str]):\n",
    "        \n",
    "        word_dic = set()\n",
    "        counter = Counter()\n",
    "        processed_sentence = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "        \n",
    "            #計算字詞頻率\n",
    "            counter.update(sentence)\n",
    "            processed_sentence.append(sentence)\n",
    "    \n",
    "        #移除頻率過小的字詞 建立word2idx與idx2word與word_frequency辭典\n",
    "        word_cnt = dict(filter(lambda x:x[1]>self.min_freq,counter.items()))\n",
    "        \n",
    "        \n",
    "        self.word2idx = {word: idx for idx, word in enumerate(word_cnt.keys(),0)}\n",
    "        self.idx2word = {idx:word for word, idx in self.word2idx.items()}\n",
    "        self.word_frequency = word_cnt.copy()\n",
    "        \n",
    "        #將文本轉為ID型式與移除文本中頻率過小的文字\n",
    "        self.processed_corpus = [[self.word2idx[word] for word in line if word in self.word2idx] for line in processed_sentence]\n",
    "        self.total_num_words = sum([len(line) for line in self.processed_corpus])\n",
    "        print(f\"Before subsampling: {self.total_num_words} words\")\n",
    "        \n",
    "        # 進行二次採樣(subsampling)\n",
    "        if self.do_subsampling:\n",
    "            ### <your code> ###\n",
    "            self.processed_corpus = [[idx for idx in line if not self.subsampling(idx)] for line in self.processed_corpus]\n",
    "            self.total_num_words = sum([len(line) for line in self.processed_corpus])\n",
    "            counter = Counter([self.idx2word[idx] for line in self.processed_corpus for idx in line])\n",
    "            word_cnt = dict(counter.items())\n",
    "            self.word_frequency = word_cnt.copy()\n",
    "            \n",
    "            print(f\"After subsampling: {self.total_num_words} words\")\n",
    "        \n",
    "        # hint: 移除空字串\n",
    "        self.processed_corpus = [[idx for idx in line]for line in self.processed_corpus if len(line) != 0]\n",
    "        \n",
    "        return self.processed_corpus, self.word2idx, self.idx2word, self.word_frequency, self.total_num_words\n",
    "    \n",
    "    def subsampling(self, idx):\n",
    "        \n",
    "        # hint: 學員可以參考講義的subsampling公式(也可自己定義一個)\n",
    "        \n",
    "        ### <your code> ###\n",
    "        p = self.t / self.word_frequency[self.idx2word[idx]] * self.total_num_words\n",
    "        p_w = math.sqrt(p) + p\n",
    "        \n",
    "        return random.uniform(0, 1) < p_w\n",
    "\n",
    "\n",
    "# 進行資料前處理\n",
    "# 這邊我們subsampling的t取1e-4\n",
    "pre_processor = PreProcessor(True, 5, True, 1e-4)\n",
    "corpus, word2idx, idx2word, word2freq, total_num_words = pre_processor.process(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfDuJuT5Kkvl"
   },
   "source": [
    "### 定義Skip-gram使用的Dataset與collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1905,
     "status": "ok",
     "timestamp": 1606320765981,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "DraniEYMKfWl"
   },
   "outputs": [],
   "source": [
    "# 客製化Dataset\n",
    "class SkipGramGetAllDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, corpus, word2freq, word2idx, idx2word, window_size, num_negatives):\n",
    "        self.corpus = corpus\n",
    "        self.word2freq = word2freq\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.window_size = window_size\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "        self.all_targets, self.all_contexts = self._get_all_contexts_targets()\n",
    "        self.all_negatives = self._get_all_negatives()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return (self.all_targets[idx], self.all_contexts[idx], self.all_negatives[idx])\n",
    "        \n",
    "    \n",
    "    def _get_all_contexts_targets(self):\n",
    "        all_targets = []\n",
    "        all_contexts = []\n",
    "        \n",
    "        for line in self.corpus:\n",
    "            if len(line) < 2*self.window_size + 1:\n",
    "                continue\n",
    "            \n",
    "            all_contexts += line[self.window_size:-self.window_size]\n",
    "            \n",
    "            for index in range(self.window_size, len(line) - self.window_size):\n",
    "                indices = list(range(max(0, index - self.window_size), min(len(line), index + self.window_size + 1)))\n",
    "                indices.remove(index)\n",
    "                all_targets.append([line[idx] for idx in indices])\n",
    "                               \n",
    "        return all_targets, all_contexts\n",
    "                               \n",
    "    \n",
    "    def _get_all_negatives(self):\n",
    "        \n",
    "        cur_exists_words = list(self.word2freq.keys())\n",
    "        sampling_weights = [self.word2freq[word]**0.75 for word in self.word2freq]\n",
    "        population = list(range(len(sampling_weights)))\n",
    "        \n",
    "        all_negatives = []\n",
    "        neg_candidate = []\n",
    "        i = 0\n",
    "        for targets in self.all_targets:\n",
    "            negatives = []\n",
    "            while len(negatives) < self.num_negatives:\n",
    "                if i == len(neg_candidate):\n",
    "                    neg_candidate = random.choices(population, sampling_weights, k=int(1e5))\n",
    "                    neg_candidate = list(map(lambda x: self.word2idx[cur_exists_words[x]], neg_candidate))\n",
    "                    i = 0\n",
    "                \n",
    "                if neg_candidate[i] not in targets:\n",
    "                    negatives.append(neg_candidate[i])\n",
    "                i += 1\n",
    "            all_negatives.append(negatives)        \n",
    "        \n",
    "        return all_negatives\n",
    "    \n",
    "# 客製化collate_fn\n",
    "def skipgram_collate(data):\n",
    "    contexts = []\n",
    "    target_negative = []\n",
    "    labels = []\n",
    "    for target, context, negative in data:\n",
    "        target_negative += [target + negative]\n",
    "        labels += [[1] * len(target) + [0] * len(negative)]\n",
    "        contexts += [context]\n",
    "    \n",
    "    return torch.tensor(contexts), torch.tensor(target_negative), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s94kJ0lKKzG5"
   },
   "source": [
    "### 定義Skip-gram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1606320766292,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "kyyQyLxcKpv1"
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.in_embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.out_embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        \n",
    "    def forward(self, contexts, targets):\n",
    "        v = self.in_embedding(contexts)\n",
    "        u = self.in_embedding(targets)\n",
    "        \n",
    "        # do dot product to get output\n",
    "        pred = torch.matmul(v[:,None,:],u.permute(0,2,1))\n",
    "        \n",
    "        return pred.squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHZIFz7yK5An"
   },
   "source": [
    "### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 13745,
     "status": "ok",
     "timestamp": 1606320780465,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "Hr4sVBd8K10T"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "verbose = True\n",
    "num_epochs = 50\n",
    "batch_size = 512\n",
    "embed_size = 100\n",
    "lr = 0.01\n",
    "\n",
    "model = SkipGram(len(word2idx), embed_size)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.BCEWithLogitsLoss(reduction = 'mean')\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "dataset = SkipGramGetAllDataset(corpus, word2freq, word2idx, idx2word, 1, 5)\n",
    "loader = DataLoader(dataset, batch_size = batch_size, shuffle=True, collate_fn = skipgram_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219482,
     "status": "ok",
     "timestamp": 1606321001876,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "sE28LW2_LB0I",
    "outputId": "4cf7c434-b5b8-4e73-eb3c-c2735ef0d17c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Batch: 501/714.251953125 Loss: 1.34518\n",
      "Epoch: 1/50, Loss: 1.16775\n",
      "Epoch: 2/50, Batch: 501/714.251953125 Loss: 0.69558\n",
      "Epoch: 2/50, Loss: 0.69192\n",
      "Epoch: 3/50, Batch: 501/714.251953125 Loss: 0.67268\n",
      "Epoch: 3/50, Loss: 0.67283\n",
      "Epoch: 4/50, Batch: 501/714.251953125 Loss: 0.66786\n",
      "Epoch: 4/50, Loss: 0.66887\n",
      "Epoch: 5/50, Batch: 501/714.251953125 Loss: 0.66669\n",
      "Epoch: 5/50, Loss: 0.66764\n",
      "Epoch: 6/50, Batch: 501/714.251953125 Loss: 0.66608\n",
      "Epoch: 6/50, Loss: 0.66709\n",
      "Epoch: 7/50, Batch: 501/714.251953125 Loss: 0.66547\n",
      "Epoch: 7/50, Loss: 0.66668\n",
      "Epoch: 8/50, Batch: 501/714.251953125 Loss: 0.66548\n",
      "Epoch: 8/50, Loss: 0.66620\n",
      "Epoch: 9/50, Batch: 501/714.251953125 Loss: 0.66490\n",
      "Epoch: 9/50, Loss: 0.66573\n",
      "Epoch: 10/50, Batch: 501/714.251953125 Loss: 0.66430\n",
      "Epoch: 10/50, Loss: 0.66525\n",
      "Epoch: 11/50, Batch: 501/714.251953125 Loss: 0.66361\n",
      "Epoch: 11/50, Loss: 0.66477\n",
      "Epoch: 12/50, Batch: 501/714.251953125 Loss: 0.66330\n",
      "Epoch: 12/50, Loss: 0.66439\n",
      "Epoch: 13/50, Batch: 501/714.251953125 Loss: 0.66287\n",
      "Epoch: 13/50, Loss: 0.66383\n",
      "Epoch: 14/50, Batch: 501/714.251953125 Loss: 0.66282\n",
      "Epoch: 14/50, Loss: 0.66351\n",
      "Epoch: 15/50, Batch: 501/714.251953125 Loss: 0.66246\n",
      "Epoch: 15/50, Loss: 0.66329\n",
      "Epoch: 16/50, Batch: 501/714.251953125 Loss: 0.66181\n",
      "Epoch: 16/50, Loss: 0.66285\n",
      "Epoch: 17/50, Batch: 501/714.251953125 Loss: 0.66173\n",
      "Epoch: 17/50, Loss: 0.66262\n",
      "Epoch: 18/50, Batch: 501/714.251953125 Loss: 0.66131\n",
      "Epoch: 18/50, Loss: 0.66241\n",
      "Epoch: 19/50, Batch: 501/714.251953125 Loss: 0.66109\n",
      "Epoch: 19/50, Loss: 0.66212\n",
      "Epoch: 20/50, Batch: 501/714.251953125 Loss: 0.66115\n",
      "Epoch: 20/50, Loss: 0.66196\n",
      "Epoch: 21/50, Batch: 501/714.251953125 Loss: 0.66047\n",
      "Epoch: 21/50, Loss: 0.66168\n",
      "Epoch: 22/50, Batch: 501/714.251953125 Loss: 0.66051\n",
      "Epoch: 22/50, Loss: 0.66148\n",
      "Epoch: 23/50, Batch: 501/714.251953125 Loss: 0.66038\n",
      "Epoch: 23/50, Loss: 0.66134\n",
      "Epoch: 24/50, Batch: 501/714.251953125 Loss: 0.66002\n",
      "Epoch: 24/50, Loss: 0.66123\n",
      "Epoch: 25/50, Batch: 501/714.251953125 Loss: 0.66010\n",
      "Epoch: 25/50, Loss: 0.66100\n",
      "Epoch: 26/50, Batch: 501/714.251953125 Loss: 0.65992\n",
      "Epoch: 26/50, Loss: 0.66080\n",
      "Epoch: 27/50, Batch: 501/714.251953125 Loss: 0.65989\n",
      "Epoch: 27/50, Loss: 0.66081\n",
      "Epoch: 28/50, Batch: 501/714.251953125 Loss: 0.65989\n",
      "Epoch: 28/50, Loss: 0.66060\n",
      "Epoch: 29/50, Batch: 501/714.251953125 Loss: 0.65958\n",
      "Epoch: 29/50, Loss: 0.66057\n",
      "Epoch: 30/50, Batch: 501/714.251953125 Loss: 0.65956\n",
      "Epoch: 30/50, Loss: 0.66046\n",
      "Epoch: 31/50, Batch: 501/714.251953125 Loss: 0.65934\n",
      "Epoch: 31/50, Loss: 0.66037\n",
      "Epoch: 32/50, Batch: 501/714.251953125 Loss: 0.65936\n",
      "Epoch: 32/50, Loss: 0.66015\n",
      "Epoch: 33/50, Batch: 501/714.251953125 Loss: 0.65958\n",
      "Epoch: 33/50, Loss: 0.66019\n",
      "Epoch: 34/50, Batch: 501/714.251953125 Loss: 0.65900\n",
      "Epoch: 34/50, Loss: 0.66007\n",
      "Epoch: 35/50, Batch: 501/714.251953125 Loss: 0.65893\n",
      "Epoch: 35/50, Loss: 0.65997\n",
      "Epoch: 36/50, Batch: 501/714.251953125 Loss: 0.65942\n",
      "Epoch: 36/50, Loss: 0.65993\n",
      "Epoch: 37/50, Batch: 501/714.251953125 Loss: 0.65849\n",
      "Epoch: 37/50, Loss: 0.65976\n",
      "Epoch: 38/50, Batch: 501/714.251953125 Loss: 0.65878\n",
      "Epoch: 38/50, Loss: 0.65983\n",
      "Epoch: 39/50, Batch: 501/714.251953125 Loss: 0.65892\n",
      "Epoch: 39/50, Loss: 0.65982\n",
      "Epoch: 40/50, Batch: 501/714.251953125 Loss: 0.65855\n",
      "Epoch: 40/50, Loss: 0.65959\n",
      "Epoch: 41/50, Batch: 501/714.251953125 Loss: 0.65856\n",
      "Epoch: 41/50, Loss: 0.65958\n",
      "Epoch: 42/50, Batch: 501/714.251953125 Loss: 0.65861\n",
      "Epoch: 42/50, Loss: 0.65950\n",
      "Epoch: 43/50, Batch: 501/714.251953125 Loss: 0.65857\n",
      "Epoch: 43/50, Loss: 0.65954\n",
      "Epoch: 44/50, Batch: 501/714.251953125 Loss: 0.65861\n",
      "Epoch: 44/50, Loss: 0.65937\n",
      "Epoch: 45/50, Batch: 501/714.251953125 Loss: 0.65831\n",
      "Epoch: 45/50, Loss: 0.65938\n",
      "Epoch: 46/50, Batch: 501/714.251953125 Loss: 0.65837\n",
      "Epoch: 46/50, Loss: 0.65932\n",
      "Epoch: 47/50, Batch: 501/714.251953125 Loss: 0.65851\n",
      "Epoch: 47/50, Loss: 0.65933\n",
      "Epoch: 48/50, Batch: 501/714.251953125 Loss: 0.65847\n",
      "Epoch: 48/50, Loss: 0.65915\n",
      "Epoch: 49/50, Batch: 501/714.251953125 Loss: 0.65826\n",
      "Epoch: 49/50, Loss: 0.65911\n",
      "Epoch: 50/50, Batch: 501/714.251953125 Loss: 0.65828\n",
      "Epoch: 50/50, Loss: 0.65926\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "lst_loss = []\n",
    "model.train()\n",
    "for epc in range(num_epochs):\n",
    "    batch_loss = 0\n",
    "\n",
    "    for i, (contexts, target_negative, labels) in enumerate(loader, 1):\n",
    "        # hint: 開始訓練前要先將optimizer的梯度歸零\n",
    "        \n",
    "        ### <your code> ###\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_cuda:\n",
    "            ### <your code> ###\n",
    "            contexts = contexts.cuda()\n",
    "            target_negative = target_negative.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        pred = model(contexts, target_negative)\n",
    "        loss = criterion(pred.float(), labels.float())\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(f\"Epoch: {epc + 1}/{num_epochs}, Batch: {i+1}/{len(dataset)/batch_size} Loss: {batch_loss / i:.5f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Epoch: {epc + 1}/{num_epochs}, Loss: {batch_loss / i:.5f}\")\n",
    "    \n",
    "    lst_loss.append(batch_loss/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1606321013487,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "y0rt5W2ELLvP",
    "outputId": "b497edcc-fc8e-47d3-b3ff-c574d42ff581"
   },
   "outputs": [],
   "source": [
    "# visualization loss\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lst_loss, marker='s')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Word2Vec Skip-gram Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1606321030038,
     "user": {
      "displayName": "劉冠宏",
      "photoUrl": "",
      "userId": "10277899974318815441"
     },
     "user_tz": -480
    },
    "id": "43pOYRh-MX_F",
    "outputId": "3de0675b-47af-462d-c927-f14a62933956"
   },
   "outputs": [],
   "source": [
    "#計算字詞相似度\n",
    "\n",
    "def get_similarity(word, top_k, model, word2idx, idx2word):\n",
    "    W = (model.in_embedding.weight.data + model.out_embedding.weight.data) / 2\n",
    "    idx = word2idx.get(word, None)\n",
    "    \n",
    "    if not idx:\n",
    "        # 當出現不在字典中的字詞時，顯示Out of vocabulary error\n",
    "        raise ValueError(\"Out of vocabulary\")\n",
    "    else:\n",
    "        x = W[idx]\n",
    "        \n",
    "        # 使用cosine相似計算字詞間的相似程度\n",
    "        cos = torch.matmul(W, x) / (torch.sum(W * W, dim=-1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "        _, topk = torch.topk(cos, top_k+1)\n",
    "        \n",
    "        for i in topk[1:]:\n",
    "            print(f\"cosine sim={cos[int(i)]:.3f}: {idx2word[int(i)]}.\")\n",
    "\n",
    "get_similarity('love', 4, model, word2idx, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_tL9g0oMcCT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNRbZbSHSpiMTWmQCCagSqg",
   "collapsed_sections": [],
   "name": "word2vec高速化_作業解答.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
